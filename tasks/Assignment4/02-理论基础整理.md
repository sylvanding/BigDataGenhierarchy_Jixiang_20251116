# Assignment 4 理论基础整理

本文档整理了Assignment 4所需的理论基础，包括MVPT、CGHT和完全线性划分的原理及其在度量空间中的应用。

## 目录

- [一、多Pivot树状索引概述](#一多pivot树状索引概述)
- [二、MVP树（Multiple Vantage Point Tree）](#二mvp树multiple-vantage-point-tree)
- [三、完全广义超平面树（Complete GHT）](#三完全广义超平面树complete-ght)
- [四、完全线性划分](#四完全线性划分)
- [五、三种索引的理论对比](#五三种索引的理论对比)
- [六、支撑点空间模型](#六支撑点空间模型)
- [七、参考文献](#七参考文献)

---

## 一、多Pivot树状索引概述

### 1.1 从单/双Pivot到多Pivot

在Assignment 3中，我们实现了：

- **GH树**：使用2个pivot进行超平面划分
- **VP树**：使用1个pivot进行球形划分

这些基础索引在小规模数据上表现良好，但在更大规模或更高维度数据上，剪枝效果可能不足。**多Pivot索引**的核心思想是：

> 使用更多的pivot可以获得更多的距离信息，从而实现更精细的数据划分和更有效的查询剪枝。

### 1.2 多Pivot索引的挑战

虽然更多的pivot意味着更多信息，但也带来挑战：

1. **划分复杂度增加**：k个pivot可能产生$O(f^k)$个子区域
2. **距离计算增加**：需要计算到更多pivot的距离
3. **存储开销增加**：需要存储更多的边界信息
4. **划分不平衡风险**：部分子区域可能为空或很小

### 1.3 三种多Pivot划分策略

| 策略 | 代表索引 | 核心思想 | 信息利用 |
|------|----------|----------|----------|
| 球形嵌套划分 | MVPT | 每个pivot独立进行球形划分 | 距离值 |
| 超平面组合划分 | CGHT | 利用pivot对的距离差 | 距离差 |
| 线性划分 | 完全线性划分 | 在支撑点空间中线性切分 | 距离向量 |

---

## 二、MVP树（Multiple Vantage Point Tree）

### 2.1 基本思想

MVP树（Multiple Vantage Point Tree）由Bozkaya和Ozsoyoglu于1999年提出[1]，是VP树的自然扩展：

- **VP树**：1个pivot，划分为2个部分，记为MVP(1,2)
- **MVP树**：k个pivot，每个pivot划分为f个部分，记为MVP(k,f)

对于MVP(3,2)树：

- 使用3个pivot：$p_1, p_2, p_3$
- 每个pivot将数据划分为2个部分（内球/外球）
- 总共产生$2^3 = 8$个子区域

### 2.2 数据结构

#### 2.2.1 内部节点结构

```
MVPTInternalNode {
    pivot[];           // k个支撑点
    children[];        // f^k 棵子树
    lowerBound[][];    // 每棵子树到每个支撑点的距离下界
    upperBound[][];    // 每棵子树到每个支撑点的距离上界
}
```

对于3-pivot MVPT (k=3, f=2)：

- `pivot[3]`：3个支撑点
- `children[8]`：8棵子树
- `lowerBound[8][3]`：8个子树 × 3个pivot的下界
- `upperBound[8][3]`：8个子树 × 3个pivot的上界

#### 2.2.2 子树编码

子树索引使用二进制编码：

| 索引 | 二进制 | p1区域 | p2区域 | p3区域 |
|------|--------|--------|--------|--------|
| 0 | 000 | 内球 | 内球 | 内球 |
| 1 | 001 | 外球 | 内球 | 内球 |
| 2 | 010 | 内球 | 外球 | 内球 |
| 3 | 011 | 外球 | 外球 | 内球 |
| 4 | 100 | 内球 | 内球 | 外球 |
| 5 | 101 | 外球 | 内球 | 外球 |
| 6 | 110 | 内球 | 外球 | 外球 |
| 7 | 111 | 外球 | 外球 | 外球 |

### 2.3 批建算法

**Algorithm: MVPTBulkload(3,2)**

```
输入: data[1...n], MaxLeafSize, k=3, f=2
输出: MVPT根节点

1: if n ≤ MaxLeafSize
2:     return new LeafNode(data)
3: end if

4: pivots[] = selectPivots(data, k=3)    // 选择3个支撑点
5: data = data - {pivots}

6: // 计算所有数据到各pivot的距离
7: for each d in data
8:     dist[d] = [d(d, pivots[0]), d(d, pivots[1]), d(d, pivots[2])]
9: end for

10: // 计算划分半径（各pivot的中位数距离）
11: for i = 0 to 2
12:     splitRadius[i] = median(dist[*][i])
13: end for

14: // 将数据分配到8个子集
15: partitions[0..7] = ∅
16: for each d in data
17:     idx = 0
18:     if dist[d][0] > splitRadius[0]: idx |= 1
19:     if dist[d][1] > splitRadius[1]: idx |= 2
20:     if dist[d][2] > splitRadius[2]: idx |= 4
21:     partitions[idx].add(d)
22: end for

23: // 计算每个子集的距离范围
24: for i = 0 to 7
25:     for j = 0 to 2
26:         lowerBound[i][j] = min(dist[d][j] for d in partitions[i])
27:         upperBound[i][j] = max(dist[d][j] for d in partitions[i])
28:     end for
29: end for

30: // 递归构建子树
31: for i = 0 to 7
32:     children[i] = MVPTBulkload(partitions[i])
33: end for

34: return MVPTInternalNode(pivots, children, lowerBound, upperBound)
```

### 2.4 范围查询算法

**Algorithm: MVPTRangeSearch**

```
输入: node, q, r
输出: 范围查询结果集 result

1: if node is LeafNode
2:     return linearScan(node.data, q, r)
3: end if

4: result = ∅

5: // 检查pivot是否为查询结果
6: for each p in node.pivots
7:     if d(p, q) ≤ r: result.add(p)
8: end for

9: // 计算查询对象到各pivot的距离
10: dq[] = [d(q, node.pivots[0]), d(q, node.pivots[1]), d(q, node.pivots[2])]

11: // 检查每个子树
12: for i = 0 to 7
13:     canPrune = false
14:     fullyContained = false
15:     
16:     for j = 0 to 2
17:         L = node.lowerBound[i][j]
18:         U = node.upperBound[i][j]
19:         
20:         // 排除规则：查询球与子树距离范围不相交
21:         if dq[j] + r < L or dq[j] - r > U
22:             canPrune = true
23:             break
24:         end if
25:         
26:         // 包含规则：子树完全在查询球内
27:         if dq[j] + U ≤ r
28:             fullyContained = true
29:             break
30:         end if
31:     end for
32:     
33:     if fullyContained
34:         result.addAll(getAllData(node.children[i]))
35:     else if not canPrune
36:         result.addAll(MVPTRangeSearch(node.children[i], q, r))
37:     end if
38: end for

39: return result
```

### 2.5 剪枝规则详解

对于查询(q, r)和子树i，设：

- $d_j = d(q, p_j)$：查询对象到第j个pivot的距离
- $[L_{i,j}, U_{i,j}]$：子树i中数据到第j个pivot的距离范围

**排除规则**（任一成立即可排除）：

$$\exists j: d_j + r < L_{i,j} \quad \text{或} \quad d_j - r > U_{i,j}$$

**几何解释**：查询球与子树在某个pivot维度上不相交。

**包含规则**（任一成立则全包含）：

$$\exists j: d_j + U_{i,j} \leq r$$

**几何解释**：子树中所有数据到某个pivot的最大距离，加上查询对象到该pivot的距离，仍不超过查询半径。

---

## 三、完全广义超平面树（Complete GHT）

### 3.1 基本思想

完全广义超平面树（Complete General Hyper-plane Tree, CGHT）由毛睿于2014年提出[2]，核心思想是：

> **充分利用pivot对之间的距离差信息进行划分**

对于2个pivot $p_1, p_2$，原始GH树只利用了$d(x, p_1) - d(x, p_2)$的**符号**（正/负），而CGHT还利用其**大小**。

对于3个pivot，可以定义：

- $\delta_{12} = d(x, p_1) - d(x, p_2)$
- $\delta_{13} = d(x, p_1) - d(x, p_3)$
- $\delta_{23} = d(x, p_2) - d(x, p_3) = \delta_{13} - \delta_{12}$

只需要2个独立变量$(\delta_{12}, \delta_{13})$即可表示所有距离差信息。

### 3.2 数据结构

#### 3.2.1 内部节点结构

```
CGHTInternalNode {
    pivot1, pivot2, pivot3;   // 3个支撑点
    children[];               // 子树（4个或8个）
    delta12Range[][];         // 每个子树的δ12范围 [min, max]
    delta13Range[][];         // 每个子树的δ13范围 [min, max]
}
```

#### 3.2.2 划分策略

**策略A：4路划分（基于符号）**

| 索引 | $\delta_{12}$条件 | $\delta_{13}$条件 | 几何含义 |
|------|-------------------|-------------------|----------|
| 0 | < 0 | < 0 | 离$p_1$最远 |
| 1 | ≥ 0 | < 0 | 离$p_3$最远 |
| 2 | < 0 | ≥ 0 | 离$p_2$最远 |
| 3 | ≥ 0 | ≥ 0 | 离$p_1$最近 |

**策略B：8路划分（加入$\delta_{23}$）**

进一步根据$\delta_{23} = \delta_{13} - \delta_{12}$的符号划分，产生8个子区域。

### 3.3 批建算法

**Algorithm: CGHTBulkload (4路划分版)**

```
输入: data[1...n], MaxLeafSize
输出: CGHT根节点

1: if n ≤ MaxLeafSize
2:     return new LeafNode(data)
3: end if

4: (p1, p2, p3) = selectPivots(data, 3)
5: data = data - {p1, p2, p3}

6: // 计算距离差
7: for each d in data
8:     d1 = d(d, p1), d2 = d(d, p2), d3 = d(d, p3)
9:     delta12[d] = d1 - d2
10:    delta13[d] = d1 - d3
11: end for

12: // 将数据分配到4个子集
13: partitions[0..3] = ∅
14: for each d in data
15:     idx = 0
16:     if delta12[d] >= 0: idx |= 1
17:     if delta13[d] >= 0: idx |= 2
18:     partitions[idx].add(d)
19: end for

20: // 计算每个子集的delta范围
21: for i = 0 to 3
22:     delta12Range[i] = [min(delta12 in partitions[i]), 
23:                        max(delta12 in partitions[i])]
24:     delta13Range[i] = [min(delta13 in partitions[i]), 
25:                        max(delta13 in partitions[i])]
26: end for

27: // 递归构建子树
28: for i = 0 to 3
29:     children[i] = CGHTBulkload(partitions[i])
30: end for

31: return CGHTInternalNode(p1, p2, p3, children, delta12Range, delta13Range)
```

### 3.4 范围查询剪枝规则

**基于GH树剪枝规则的扩展**

回顾GH树剪枝规则（定理3-3）：

- 若$d(q, p_1) - d(q, p_2) > 2r$，排除左子树（$\delta_{12} < 0$的区域）
- 若$d(q, p_2) - d(q, p_1) > 2r$，排除右子树（$\delta_{12} \geq 0$的区域）

**CGHT的剪枝规则**：

对于查询(q, r)，设：

- $\delta_{12}^q = d(q, p_1) - d(q, p_2)$
- $\delta_{13}^q = d(q, p_1) - d(q, p_3)$

对于子树i，其$\delta_{12}$范围为$[L_{12}^i, U_{12}^i]$，$\delta_{13}$范围为$[L_{13}^i, U_{13}^i]$。

**排除条件**（任一成立即可排除）：

$$\delta_{12}^q - 2r > U_{12}^i \quad \text{或} \quad \delta_{12}^q + 2r < L_{12}^i$$
$$\delta_{13}^q - 2r > U_{13}^i \quad \text{或} \quad \delta_{13}^q + 2r < L_{13}^i$$

**推导**：

若$\delta_{12}^q - 2r > U_{12}^i$，则对于子树i中的任意数据s：
$$\delta_{12}^q > U_{12}^i + 2r \geq \delta_{12}^s + 2r$$

即$d(q, p_1) - d(q, p_2) > d(s, p_1) - d(s, p_2) + 2r$

由GH树剪枝规则的证明思路，可得$d(q, s) > r$，即s不可能是查询结果。

### 3.5 与原始GHT的关系

| 特性 | 原始GHT | CGHT |
|------|---------|------|
| Pivot数 | 2 | 3（或更多） |
| 利用信息 | $\delta_{12}$符号 | $\delta_{12}, \delta_{13}$范围 |
| 子区域数 | 2 | 4或8 |
| 剪枝精度 | 较粗 | 较细 |
| 构建开销 | 较小 | 较大 |

---

## 四、完全线性划分

### 4.1 支撑点空间基础

#### 4.1.1 支撑点空间定义

**定义**：给定度量空间$(M, d)$和k个支撑点$P = \{p_1, ..., p_k\}$，支撑点空间映射定义为：

$$F_d^P: M \to \mathbb{R}^k: x^P = F_d^P(x) = (d(x, p_1), ..., d(x, p_k))$$

对于3个pivot，数据被映射到3维空间：$(d_1, d_2, d_3)$。

#### 4.1.2 支撑点空间的性质

**性质1（距离下界）**：对于度量空间中的任意两点$x, y$：

$$d(x, y) \geq \max_{i} |d(x, p_i) - d(y, p_i)| = d_{\infty}(x^P, y^P)$$

即度量空间距离是支撑点空间切比雪夫距离的上界。

**性质2（范围查询映射）**：度量空间中以q为中心、半径r的范围查询，映射到支撑点空间后：

- 包含于以$q^P$为中心、半径r的切比雪夫球中
- 即一个边长2r的超立方体$\{x^P: \max_i |x_i^P - q_i^P| \leq r\}$

### 4.2 完全线性划分思想

**核心思想**：

既然数据已经映射到支撑点空间（多维实数空间），就可以使用传统的多维索引方法进行划分。

**线性划分**：使用线性超平面$\sum_i a_i x_i = c$对支撑点空间进行划分。

最简单的线性划分是**正交划分**：

- 按$d_1$的中位数划分
- 按$d_2$的中位数划分
- 按$d_3$的中位数划分
- 共产生$2^3 = 8$个子区域

### 4.3 数据结构

```
LinearPartitionInternalNode {
    pivot[];              // 3个支撑点
    children[];           // 8棵子树
    splitThreshold[];     // 3个维度的划分阈值
    lowerBound[][];       // 每个子树在每个维度的下界
    upperBound[][];       // 每个子树在每个维度的上界
}
```

### 4.4 批建算法

**Algorithm: LinearPartitionBulkload**

```
输入: data[1...n], MaxLeafSize
输出: 线性划分树根节点

1: if n ≤ MaxLeafSize
2:     return new LeafNode(data)
3: end if

4: pivots[] = selectPivots(data, 3)
5: data = data - {pivots}

6: // 将数据映射到支撑点空间
7: for each d in data
8:     coord[d] = [d(d, pivots[0]), d(d, pivots[1]), d(d, pivots[2])]
9: end for

10: // 计算划分阈值（各维度中位数）
11: for i = 0 to 2
12:     splitThreshold[i] = median(coord[*][i])
13: end for

14: // 将数据分配到8个子集
15: partitions[0..7] = ∅
16: for each d in data
17:     idx = 0
18:     if coord[d][0] > splitThreshold[0]: idx |= 1
19:     if coord[d][1] > splitThreshold[1]: idx |= 2
20:     if coord[d][2] > splitThreshold[2]: idx |= 4
21:     partitions[idx].add(d)
22: end for

23: // 计算每个子集的坐标范围
24: for i = 0 to 7
25:     for j = 0 to 2
26:         lowerBound[i][j] = min(coord[d][j] for d in partitions[i])
27:         upperBound[i][j] = max(coord[d][j] for d in partitions[i])
28:     end for
29: end for

30: // 递归构建子树
31: for i = 0 to 7
32:     children[i] = LinearPartitionBulkload(partitions[i])
33: end for

34: return LinearPartitionInternalNode(pivots, children, splitThreshold, 
35:                                     lowerBound, upperBound)
```

### 4.5 范围查询剪枝规则

**查询对象映射**：查询对象q映射到支撑点空间为$q^P = (d_1^q, d_2^q, d_3^q)$。

**查询区域**：在支撑点空间中，范围查询(q, r)对应的区域是以$q^P$为中心的边长2r的立方体：

$$\{(x_1, x_2, x_3): |x_i - d_i^q| \leq r, i=1,2,3\}$$

**排除规则**：对于子树i，其坐标范围为$[L_i^j, U_i^j]$，$j=0,1,2$。

若存在任意维度j使得：
$$d_j^q + r < L_i^j \quad \text{或} \quad d_j^q - r > U_i^j$$

则子树i可以排除。

**包含规则**：若对于所有维度j都有：
$$L_i^j \leq d_j^q - r \quad \text{且} \quad U_i^j \leq d_j^q + r$$

注意：这只是子树区域被查询立方体包含，不等于子树中所有数据都是查询结果。需要进一步检验。

---

## 五、三种索引的理论对比

### 5.1 划分方式对比

| 方面 | MVPT | CGHT | 完全线性划分 |
|------|------|------|--------------|
| 划分空间 | 度量空间 | 度量空间 | 支撑点空间 |
| 划分边界 | 球面 | 超平面 | 线性超平面 |
| 边界形状(度量空间) | 同心球 | 中垂面 | 复杂曲面 |
| 子区域数 | $2^k$ | $2^{k-1}$或$2^k$ | $2^k$ |

### 5.2 支撑点信息利用对比

| 方面 | MVPT | CGHT | 完全线性划分 |
|------|------|------|--------------|
| 利用的信息 | $d(x, p_i)$ | $d(x, p_i) - d(x, p_j)$ | $(d_1, d_2, d_3)$ |
| 信息类型 | 距离值 | 距离差 | 距离向量 |
| 信息独立性 | 独立使用 | 成对使用 | 联合使用 |
| 距离计算次数(查询) | k次 | k次 | k次 |

### 5.3 剪枝条件对比

**MVPT剪枝条件**：
$$\exists j: d(q, p_j) + r < L_{i,j} \text{ 或 } d(q, p_j) - r > U_{i,j}$$

**CGHT剪枝条件**：
$$\exists (j,k): \delta_{jk}^q - 2r > U_{jk}^i \text{ 或 } \delta_{jk}^q + 2r < L_{jk}^i$$

**完全线性划分剪枝条件**：
$$\exists j: d_j^q + r < L_i^j \text{ 或 } d_j^q - r > U_i^j$$

### 5.4 剪枝效果分析

**理论分析**：

1. **MVPT**：每个pivot独立判断，剪枝相对保守
2. **CGHT**：利用距离差信息，在某些分布下更敏感
3. **完全线性划分**：在支撑点空间中判断，剪枝条件直观

**影响因素**：

- 数据分布
- 查询半径
- 数据维度
- pivot选择质量

### 5.5 复杂度分析

| 复杂度 | MVPT | CGHT | 完全线性划分 |
|--------|------|------|--------------|
| 构建-距离计算 | $O(n \cdot k)$ | $O(n \cdot k)$ | $O(n \cdot k)$ |
| 构建-划分 | $O(n \cdot 2^k)$ | $O(n \cdot 2^k)$ | $O(n \cdot 2^k)$ |
| 查询-距离计算 | $O(k)$/节点 | $O(k)$/节点 | $O(k)$/节点 |
| 空间-节点信息 | $O(2^k \cdot k)$ | $O(2^k \cdot k)$ | $O(2^k \cdot k)$ |

三种索引的渐近复杂度相似，实际性能取决于常数因子和剪枝效果。

### 5.6 优缺点总结

#### MVPT

**优点**：

- 球形划分直观，易于理解
- 包含规则存在，可以批量返回结果
- 扩展自VP树，实现相对简单

**缺点**：

- 嵌套划分可能导致子区域不规则
- pivot之间的信息没有联合利用

#### CGHT

**优点**：

- 充分利用pivot对之间的距离差信息
- 剪枝规则与GH树一脉相承
- 对某些数据分布效果好

**缺点**：

- 没有包含规则（除非记录额外信息）
- 距离差信息的利用不够直观

#### 完全线性划分

**优点**：

- 在支撑点空间中划分，可利用多维索引理论
- 线性边界简洁，易于计算
- 剪枝条件清晰

**缺点**：

- 支撑点空间中的距离是度量空间距离的下界
- 可能存在假阳性（过于保守的包含判断）

---

## 六、支撑点空间模型

### 6.1 支撑点空间与度量空间的关系

**距离扭曲**：

从度量空间映射到支撑点空间，距离可能发生扭曲：

- 支撑点空间距离 ≤ 度量空间距离（使用切比雪夫距离时）
- 若支撑点选择不当，扭曲可能严重

**完全支撑点空间**：

当所有数据点都作为支撑点时，得到完全支撑点空间。此时映射是保距的（isometric）。

### 6.2 支撑点选择与划分质量

好的支撑点应该：

1. 分散分布，覆盖数据空间
2. 使数据在支撑点空间中分布均匀
3. 减小距离扭曲

常用的选择策略：

- **FFT（最远优先遍历）**：依次选择离已选点最远的点
- **最大分散度**：选择使两两距离最大化的点集
- **随机采样+优化**：从随机样本中选择最优子集

### 6.3 维度与本征维度

**本征维度**：数据的实际维度，可能远小于支撑点空间维度。

**最优支撑点个数**：一般认为与数据本征维度相近时效果最好。

对于3-pivot索引，适用于本征维度较低的数据集。

---

## 七、参考文献

[1] Bozkaya T, Ozsoyoglu M. Indexing large metric spaces for similarity search queries[J]. ACM Transactions on Database Systems (TODS), 1999, 24(3): 361-404.

[2] Mao R, Liu S, Xu H, et al. On data partitioning in tree structure metric-space indexes[C]//International Conference on Database Systems for Advanced Applications. Cham: Springer International Publishing, 2014: 141-155.

[3] Uhlmann J K. Satisfying general proximity/similarity queries with metric trees[J]. Information processing letters, 1991, 40(4): 175-179.

[4] Yianilos P. Data structures and algorithms for nearest neighbor search in general metric spaces[J]. 1993.

[5] Chávez E, Navarro G, Baeza-Yates R, et al. Searching in metric spaces[J]. ACM computing surveys (CSUR), 2001, 33(3): 273-321.

[6] 毛睿 著，《大数据泛构》，相关章节。

---

**文档版本**：v1.0  
**创建日期**：2026-01-11  
**维护者**：Jixiang Ding
