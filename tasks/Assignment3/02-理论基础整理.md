# Assignment 3 理论基础整理

本文档整理了Assignment 3所需的理论基础，包括GH树和VP树的原理、算法及其在UMAD-OriginalCode中的实现分析。

## 目录

- [一、树状索引概述](#一树状索引概述)
- [二、GH树（Generalized Hyperplane Tree）](#二gh树generalized-hyperplane-tree)
- [三、VP树（Vantage Point Tree）](#三vp树vantage-point-tree)
- [四、GHT与VPT的理论对比](#四ght与vpt的理论对比)
- [五、Pivot选择理论](#五pivot选择理论)
- [六、UMAD-OriginalCode实现分析](#六umad-originalcode实现分析)
- [七、参考文献](#七参考文献)

---

## 一、树状索引概述

### 1.1 为什么需要树状索引

**Pivot Table的局限性**：

在Assignment 2中，我们实现了Pivot Table索引。Pivot Table通过预计算数据到支撑点的距离，利用三角不等式进行剪枝。但是：

1. **距离计算多**：每次查询都需要计算查询对象到所有pivot的距离
2. **剪枝效果受限**：只能利用全局的pivot信息，不能根据数据分布进行层次化剪枝
3. **空间开销大**：需要存储所有数据到所有pivot的距离

**树状索引的优势**：

1. **层次化剪枝**：通过树的层次结构，可以提前排除整个子树
2. **局部优化**：每个内部节点可以根据其子树的数据分布选择最优的pivot
3. **距离计算少**：只计算查询对象到访问路径上节点的pivot的距离
4. **更好的可扩展性**：适合大规模数据集

### 1.2 树状索引的基本思想

**核心思想**：

1. **递归划分**：将数据集递归地划分为若干子集
2. **选择支撑点**：在每个节点选择一个或多个支撑点（pivot）
3. **数据分配**：根据数据到支撑点的关系，将数据分配到不同的子树
4. **剪枝规则**：查询时根据查询对象到支撑点的距离，决定是否需要访问子树

**树的结构**：

```
              [Root]
             p1    p2
            /  \
      [Internal]  [Internal]
        p3  p4      p5  p6
       / \          / \
   [Leaf] [Leaf] [Leaf] [Leaf]
```

### 1.3 数据划分方式分类

根据划分方式，树状索引可以分为两大类：

#### 1.3.1 超平面划分（Hyperplane Partitioning）

**代表**：GH树（Generalized Hyperplane Tree）

**原理**：

- 选择两个支撑点 p1 和 p2
- 定义超平面：{x | d(x, p1) = d(x, p2)}
- 数据划分：
  - d(x, p1) < d(x, p2) → 左子树
  - d(x, p1) ≥ d(x, p2) → 右子树

**几何直观**：

```
         p1              p2
         *               *
          \    超平面   /
           \     |     /
            \    |    /
             \   |   /
         左子树 | 右子树
```

#### 1.3.2 球形划分（Ball Partitioning）

**代表**：VP树（Vantage Point Tree）

**原理**：

- 选择一个支撑点 p
- 计算所有数据到p的距离
- 选择中位数距离r作为半径
- 数据划分：
  - d(x, p) ≤ r → 内球（inner sphere）
  - d(x, p) > r → 外球（outer sphere）

**几何直观**：

```
          r
     _____|_____
    /           \
   |      p      |  内球
    \_____*_____/
          |
     外球区域
```

---

## 二、GH树（Generalized Hyperplane Tree）

### 2.1 基本思想：超平面划分

**GH树的核心思想**（来自full.md 3.4节）：

GH树使用**超平面**来划分度量空间。给定两个支撑点 p1 和 p2，定义**广义超平面**（Generalized Hyperplane）为：

$$H(p_1, p_2) = \\{x \\in M \\ | \\ d(x, p_1) = d(x, p_2)\\}$$

这个超平面将空间划分为两个区域：

- **左区域**：$L = \\{x \\in M \\ | \\ d(x, p_1) < d(x, p_2)\\}$
- **右区域**：$R = \\{x \\in M \\ | \\ d(x, p_1) \\geq d(x, p_2)\\}$

**为什么叫"广义"超平面**？

在欧几里得空间中，满足 d(x, p1) = d(x, p2) 的点集确实形成一个超平面（垂直平分线/面）。但在一般度量空间中，这个集合可能不是平面，所以称为"广义"超平面。

### 2.2 数据结构设计

#### 2.2.1 GH树节点结构

**内部节点（Internal Node）**：

```java
class GHInternalNode {
    MetricSpaceData pivot1;    // 第一个支撑点
    MetricSpaceData pivot2;    // 第二个支撑点
    TreeNode leftChild;        // 左子树（离p1近）
    TreeNode rightChild;       // 右子树（离p2近）
    int depth;                 // 节点深度
}
```

**叶子节点（Leaf Node）**：

```java
class LeafNode {
    List<MetricSpaceData> data;  // 存储的数据对象
    int depth;                    // 节点深度
}
```

#### 2.2.2 树的性质

1. **二叉树**：GH树是二叉树，每个内部节点恰好有两个子节点
2. **支撑点数量**：每个内部节点使用2个支撑点
3. **数据存储**：数据只存储在叶子节点中
4. **平衡性**：GH树不保证平衡，取决于数据分布和pivot选择

### 2.3 批建算法原理

**算法：BuildGHTree**

```
输入：数据集 S，当前深度 depth
输出：GH树的根节点

1. 终止条件判断：
   if |S| ≤ maxLeafSize AND depth ≥ minTreeHeight:
       return LeafNode(S, depth)

2. 选择支撑点：
   (p1, p2) = SelectTwoPivots(S)

3. 数据划分：
   S_left = {x ∈ S | d(x, p1) < d(x, p2)}
   S_right = {x ∈ S | d(x, p1) ≥ d(x, p2)}

4. 递归构建子树：
   leftChild = BuildGHTree(S_left, depth + 1)
   rightChild = BuildGHTree(S_right, depth + 1)

5. 返回内部节点：
   return GHInternalNode(p1, p2, leftChild, rightChild, depth)
```

**关键设计决策**：

1. **支撑点选择策略**：
   - **随机选择**：快速但可能不优
   - **FFT（Farthest-First Traversal）**：选择距离较远的两点
   - **最大分散度**：遍历找到距离最远的两点

2. **终止条件**：
   - 数据量小于等于maxLeafSize
   - 已达到最小树高要求

3. **数据划分不均的处理**：
   - 如果所有数据都分到一侧，强制创建叶子节点
   - 记录警告信息

**时间复杂度分析**：

- 单次划分：O(n)，其中n是当前节点的数据量
- 总时间复杂度：O(n log n)（平衡情况）到 O(n²)（极度不平衡）

### 2.4 范围查询算法原理

**查询定义**：

给定查询对象 q 和查询半径 r，找出所有满足 d(q, x) ≤ r 的数据对象 x。

**GH树的剪枝规则**（核心！）：

在GH树的内部节点，有两个支撑点 p1 和 p2。设：

- d1 = d(q, p1)：查询对象到p1的距离
- d2 = d(q, p2)：查询对象到p2的距离

**剪枝规则1**（排除左子树）：
如果 **d1 - d2 > 2r**，则可以排除左子树。

**证明**：

- 左子树中的任意数据点 x 满足：d(x, p1) < d(x, p2)
- 即：d(x, p2) - d(x, p1) > 0
- 由三角不等式：
  - d(q, x) ≥ |d(q, p1) - d(x, p1)| = |d1 - d(x, p1)|
  - d(q, x) ≥ |d(q, p2) - d(x, p2)| = |d2 - d(x, p2)|
- 如果 d1 - d2 > 2r，则：
  - d1 - d(x, p1) + d(x, p2) - d2 > 2r
  - 由于 d(x, p2) > d(x, p1)，可推出 d(q, x) > r
- 因此可以排除左子树

**剪枝规则2**（排除右子树）：
如果 **d2 - d1 > 2r**，则可以排除右子树。

**算法：GHRangeQuery**

```
输入：查询对象 q，查询半径 r，当前节点 node
输出：满足查询条件的数据对象列表

1. if node is LeafNode:
       result = []
       for each x in node.data:
           if d(q, x) ≤ r:
               add x to result
       return result

2. if node is GHInternalNode:
       result = []
       d1 = d(q, node.p1)
       d2 = d(q, node.p2)
       
       // 检查是否访问左子树
       if NOT (d1 - d2 > 2r):
           result.addAll(GHRangeQuery(q, r, node.leftChild))
       
       // 检查是否访问右子树
       if NOT (d2 - d1 > 2r):
           result.addAll(GHRangeQuery(q, r, node.rightChild))
       
       return result
```

**性能优势**：

通过剪枝，可以避免访问大量不相关的子树，减少：

1. 距离计算次数
2. 节点访问次数
3. 查询响应时间

### 2.5 kNN查询算法原理

**查询定义**：

找出与查询对象 q 距离最近的 k 个数据对象。

**算法策略**：

1. 使用**优先队列**维护当前找到的k个最近邻
2. 维护**当前k近邻的最大距离** r_k 作为动态查询半径
3. 应用GH树的剪枝规则，但使用动态更新的 r_k

**算法：GHKNNQuery**

```
输入：查询对象 q，近邻数 k，当前节点 node
输出：k个最近邻及其距离

1. 初始化优先队列（最大堆） knnQueue
   r_k = ∞  // 当前k近邻的最大距离

2. function TraverseGHTree(node):
       if node is LeafNode:
           for each x in node.data:
               dist = d(q, x)
               if |knnQueue| < k:
                   add (x, dist) to knnQueue
                   update r_k
               else if dist < r_k:
                   remove max from knnQueue
                   add (x, dist) to knnQueue
                   update r_k
       
       if node is GHInternalNode:
           d1 = d(q, node.p1)
           d2 = d(q, node.p2)
           
           // 使用动态r_k进行剪枝
           if NOT (d1 - d2 > 2 * r_k):
               TraverseGHTree(node.leftChild)
           if NOT (d2 - d1 > 2 * r_k):
               TraverseGHTree(node.rightChild)

3. TraverseGHTree(root)
4. return knnQueue
```

**优化策略**：

1. **优先访问更可能包含近邻的子树**：比较 d1 和 d2，优先访问较小的那一侧
2. **动态剪枝**：随着找到更近的邻居，r_k 不断缩小，剪枝效果越来越好
3. **早停策略**：某些情况下可以提前终止搜索

---

## 三、VP树（Vantage Point Tree）

### 3.1 基本思想：球形划分

**VP树的核心思想**（来自full.md 3.5节）：

VP树使用**球形区域**来划分度量空间。给定一个支撑点（vantage point）p 和半径 r，空间被划分为：

- **内球**：$B_{inner} = \\{x \\in M \\ | \\ d(x, p) \\leq r\\}$
- **外球**：$B_{outer} = \\{x \\in M \\ | \\ d(x, p) > r\\}$

**与GH树的本质区别**：

- GH树：使用**两个**支撑点定义超平面
- VP树：使用**一个**支撑点定义球形边界

### 3.2 数据结构设计

#### 3.2.1 VP树节点结构

**内部节点**：

```java
class VPInternalNode {
    MetricSpaceData pivot;           // 支撑点（vantage point）
    List<TreeNode> children;         // 子节点列表（通常2个：内球、外球）
    List<DistanceRange> ranges;      // 每个子树的距离范围
    int depth;
    
    class DistanceRange {
        double lower;  // 该子树中数据到pivot的最小距离
        double upper;  // 该子树中数据到pivot的最大距离
    }
}
```

**关键信息**：

每个子树存储了其中数据到pivot的距离范围 [lower, upper]，这是VP树高效剪枝的关键。

### 3.3 批建算法原理

**算法：BuildVPTree**

```
输入：数据集 S，当前深度 depth
输出：VP树的根节点

1. 终止条件判断：
   if |S| ≤ maxLeafSize AND depth ≥ minTreeHeight:
       return LeafNode(S, depth)

2. 选择支撑点：
   p = SelectVantagePoint(S)

3. 计算所有数据到p的距离：
   distances = [(x, d(x, p)) for x in S if x ≠ p]

4. 按距离排序：
   sort distances by distance value

5. 数据划分（二分）：
   mid = |distances| / 2
   S_inner = {x | (x, d) in distances[0:mid]}
   S_outer = {x | (x, d) in distances[mid:end]}

6. 计算距离范围：
   range_inner = [min(distances[0:mid]), max(distances[0:mid])]
   range_outer = [min(distances[mid:end]), max(distances[mid:end])]

7. 递归构建子树：
   child_inner = BuildVPTree(S_inner, depth + 1)
   child_outer = BuildVPTree(S_outer, depth + 1)

8. 返回内部节点：
   return VPInternalNode(p, [child_inner, child_outer], 
                         [range_inner, range_outer], depth)
```

**关键设计决策**：

1. **Vantage Point选择**：
   - **随机选择**：简单高效
   - **采样+最大方差**：从采样中选择方差最大的点
   - **FFT**：选择离参考点最远的点

2. **划分策略**：
   - **中位数划分**：保证两个子树大小相近（平衡）
   - **多路划分**：可以划分为3个或更多子区域

3. **距离范围的重要性**：
   - 准确记录每个子树的距离范围
   - 用于查询时的剪枝判断

**时间复杂度**：

- 计算距离：O(n)
- 排序：O(n log n)
- 递归：T(n) = 2T(n/2) + O(n log n) = O(n log² n)

### 3.4 范围查询算法原理

**VP树的剪枝规则**（核心！）：

在VP树的内部节点，有一个支撑点 p 和子树i的距离范围 [Li, Ui]。设：

- dq = d(q, p)：查询对象到p的距离
- r：查询半径

**剪枝规则1**（内侧剪枝）：
如果 **dq + r < Li**，则可以排除子树i。

**几何解释**：

- 查询球的最远点到p的距离为 dq + r
- 如果这个距离小于子树i中最近点到p的距离 Li
- 说明查询球和子树i完全不相交

**剪枝规则2**（外侧剪枝）：
如果 **dq - r > Ui**，则可以排除子树i。

**几何解释**：

- 查询球的最近点到p的距离为 dq - r
- 如果这个距离大于子树i中最远点到p的距离 Ui
- 说明查询球和子树i完全不相交

**算法：VPRangeQuery**

```
输入：查询对象 q，查询半径 r，当前节点 node
输出：满足查询条件的数据对象列表

1. if node is LeafNode:
       result = []
       for each x in node.data:
           if d(q, x) ≤ r:
               add x to result
       return result

2. if node is VPInternalNode:
       result = []
       dq = d(q, node.p)
       
       for i = 0 to node.children.length - 1:
           L_i = node.ranges[i].lower
           U_i = node.ranges[i].upper
           
           // 检查剪枝条件
           if NOT (dq + r < L_i OR dq - r > U_i):
               result.addAll(VPRangeQuery(q, r, node.children[i]))
       
       return result
```

### 3.5 kNN查询算法原理

与GH树的kNN查询类似，但使用VP树的剪枝规则：

```
if NOT (dq + r_k < L_i OR dq - r_k > U_i):
    visit child i
```

**优化**：

- 优先访问包含查询点的子树（dq ∈ [L_i, U_i]）
- 按子树距离范围的中点与dq的距离排序

---

## 四、GHT与VPT的理论对比

### 4.1 数据划分方式对比

| 特性 | GH树 | VP树 |
|-----|------|------|
| 划分方式 | 超平面划分 | 球形划分 |
| 支撑点数量 | 2个 | 1个 |
| 几何形状 | 由两点等距平面分割 | 由同心球分割 |
| 维度敏感性 | 中等 | 较高 |

### 4.2 支撑点使用对比

| 特性 | GH树 | VP树 |
|-----|------|------|
| 每节点pivot数 | 2 | 1 |
| pivot作用 | 定义超平面 | 定义球心 |
| 距离计算 | 需计算到2个pivot的距离 | 只需计算到1个pivot的距离 |
| 剪枝信息 | 利用两个距离的差值 | 利用距离范围 |

### 4.3 空间划分特性对比

**GH树的优势**：

1. 划分更"对称"，在某些数据分布下更均匀
2. 超平面划分在低维空间中效果好
3. 对数据分布的适应性较好

**GH树的劣势**：

1. 需要2个pivot，构建开销略大
2. 剪枝规则相对简单，剪枝效果可能不如VP树

**VP树的优势**：

1. 只需1个pivot，构建更快
2. 距离范围信息提供更精确的剪枝
3. 在高维空间中表现相对稳定

**VP树的劣势**：

1. 球形划分可能导致空间浪费
2. 高维情况下"维度灾难"明显
3. 对数据分布敏感，聚类数据表现不佳

### 4.4 理论性能对比

| 性能指标 | GH树 | VP树 |
|---------|------|------|
| 构建时间 | O(n log² n) | O(n log² n) |
| 查询时间（平均） | O(log n + k) | O(log n + k) |
| 查询时间（最坏） | O(n) | O(n) |
| 空间复杂度 | O(n) | O(n) |
| pivot选择开销 | 较高（需选2个） | 较低（只需1个） |

**理论结论**：

- 两种树的时间复杂度在理论上相近
- 实际性能取决于：
  1. 数据分布特征
  2. Pivot选择策略
  3. 查询模式
  4. 数据维度

---

## 五、Pivot选择理论

### 5.1 Pivot选择的重要性

**为什么Pivot选择很重要？**

1. **影响数据划分质量**：好的pivot能使数据划分更均匀
2. **影响剪枝效果**：好的pivot能提供更有效的剪枝边界
3. **影响树的平衡性**：影响树的高度和查询效率

### 5.2 常见Pivot选择策略

#### 5.2.1 随机选择（Random Selection）

**方法**：从数据集中随机选择

**优点**：

- 实现简单
- 时间复杂度O(1)
- 不需要额外的距离计算

**缺点**：

- 可能选到"不好"的pivot
- 划分质量不稳定
- 性能波动大

**适用场景**：

- 快速原型
- 数据分布较均匀
- 对性能要求不高

#### 5.2.2 FFT（Farthest-First Traversal）

**方法**：

1. 随机选择第一个pivot p1
2. 选择离p1最远的点作为p2
3. （对于多个pivot）选择离已选pivot最远的点

**优点**：

- 选择的pivot分散度大
- 划分质量较好
- 性能稳定

**缺点**：

- 需要O(n)的距离计算
- 对outlier敏感

**GH树中的FFT**：

```
p1 = random point from S
p2 = argmax_{x∈S} d(x, p1)
```

**VP树中的FFT**：

```
ref = random point from S
p = argmax_{x∈S} d(x, ref)
```

#### 5.2.3 最大分散度（Maximum Spread）

**方法**：找到数据集中距离最大的两个点

**实现**：

```
maxDist = -∞
for i = 1 to n:
    for j = i+1 to n:
        if d(S[i], S[j]) > maxDist:
            maxDist = d(S[i], S[j])
            p1 = S[i]
            p2 = S[j]
```

**优化**：采样以减少计算量

```
sample = random sample from S (size = min(100, |S|))
find (p1, p2) with max distance in sample
```

**优点**：

- 理论上最优
- 划分效果最好

**缺点**：

- 时间复杂度O(n²)
- 实际中采样使用

### 5.3 统一的Pivot选择策略

**为了公平对比GHT和VPT，需要使用统一的Pivot选择策略**：

**方案1**：预先选择全局pivot集合

```
GlobalPivots = SelectPivots(全部数据, 需要的总数)
GHT和VPT按相同顺序使用这些pivot
```

**方案2**：使用相同的随机种子

```
Random rand = new Random(固定种子)
GHT和VPT使用相同的rand对象
```

**方案3**：在每个对应节点使用相同的局部数据选择pivot

```
对于GHT的节点N_gh和VPT的节点N_vp
如果它们包含相同的数据集
则应该选择相同的pivot
```

---

## 六、UMAD-OriginalCode实现分析

### 6.1 代码结构概览

UMAD-OriginalCode的树状索引实现位于：

```
src/main/java/index/structure/
├── AbstractIndex.java           # 索引抽象基类
├── Index.java                   # 索引接口
├── Node.java                    # 节点接口
├── InternalNode.java            # 内部节点抽象类
├── LeafNode.java                # 叶子节点类
├── GHIndex.java                 # GH树实现
├── GHInternalNode.java          # GH树内部节点
├── GHPartitionResults.java      # GH树划分结果
├── VPIndex.java                 # VP树实现
├── VPInternalNode.java          # VP树内部节点
└── VPPartitionResults.java      # VP树划分结果
```

### 6.2 GHIndex实现分析

**核心类**：`GHIndex.java`

**关键方法**：

1. **构造函数**：

```java
public GHIndex(String indexPrefix, List<? extends IndexObject> data, 
               Metric metric, int maxLeafSize, int numPivot, int numPartitions, 
               PivotSelectionMethod pivotSelectionMethod, 
               PartitionMethod partitionMethod)
```

**限制**：

- numPivot 必须为 2
- numPartitions 必须为 2

2. **pivotSelection方法**：

```java
int[] pivotSelection(Metric metric, List<? extends IndexObject> candidateSet, 
                     List<? extends IndexObject> evaluationSet, int numPivot)
```

调用`pivotSelectionMethod.selectPivots()`选择2个pivot

3. **partition方法**：

```java
PartitionResults partition(Metric metric, IndexObject[] pivotSet, 
                          List<? extends IndexObject> data, int numPartitons)
```

调用`partitionMethod.partition()`进行数据划分

### 6.3 GHPartitionMethods实现分析

**位置**：`algorithms/datapartition/GHPartitionMethods.java`

**核心逻辑**：

```java
public enum GHPartitionMethods implements PartitionMethod {
    GH {
        public PartitionResults partition(...) {
            // 创建左右两个数据列表
            ArrayList<IndexObject> a1 = new ArrayList<>();  // 左子树
            ArrayList<IndexObject> a2 = new ArrayList<>();  // 右子树
            
            // 遍历数据进行划分
            for(int i = first; i < first + size; i++) {
                IndexObject d = data.get(i);
                dis1 = metric.getDistance(d, pivots[0]);
                dis2 = metric.getDistance(d, pivots[1]);
                
                if(dis1 < dis2) {
                    a1.add(d);  // 离p1近 -> 左子树
                } else {
                    a2.add(d);  // 离p2近 -> 右子树
                }
            }
            
            return new GHPartitionResults(subDataList, pivots);
        }
    }
}
```

**特点**：

- 简单直接的超平面划分
- 严格按照 d1 < d2 进行划分
- 不处理极端情况

### 6.4 VPIndex实现分析

**核心类**：`VPIndex.java`

**关键特性**：

- 支持多个pivot（不限于1个）
- 支持多路划分（不限于2路）
- 支持三种模式：LOCAL、GLOBAL、MIX

**构造函数**：

```java
public VPIndex(String indexPrefix, List<? extends IndexObject> data, 
               Metric metric, int maxLeafSize, int numPivot, int numPartitions,
               HierarchicalPivotSelectionMode hierarchicalPivotSelectionMode,
               PivotSelectionMethod pivotSelectionMethod, 
               PartitionMethod partitionMethod)
```

### 6.5 VPPartitionMethods实现分析

**位置**：`algorithms/datapartition/VPPartitionMethods.java`

**支持两种划分方法**：

#### 6.5.1 BALANCED划分

**核心思想**：平衡划分，确保每个子树大小相近

**关键步骤**：

1. 计算所有数据到pivot的距离
2. 按距离排序
3. 根据中位数划分为多个区域
4. 记录每个区域的距离范围 [lower, upper]

**代码片段**：

```java
// 计算距离并封装
DoubleIndexObjectPair[] wrapper = new DoubleIndexObjectPair[size];
for (int i = first; i < size; i++)
    wrapper[i] = new DoubleIndexObjectPair(0, data.get(i));

// 对每个pivot进行处理
for (int i = 0; i < numPivots; i++) {
    // 计算到当前pivot的距离
    for (int j = 0; j < size; j++)
        wrapper[j].setDouble(metric.getDistance(pivots[i], wrapper[j].getObject()));
    
    // 排序
    Arrays.sort(wrapper, clusterOffset[j], clusterOffset[j + 1], 
                DoubleIndexObjectPair.DoubleComparator);
    
    // 计算中位数并划分
    // ...
}

// 返回结果，包含距离范围
return new VPPartitionResults(subDataList, pivots, lower, upper);
```

#### 6.5.2 CLUSTERINGKMEANS划分

基于K-means聚类的划分方法，更复杂但可能效果更好。

### 6.6 查询实现分析

#### 6.6.1 GHRangeCursor

**位置**：`index/search/GHRangeCursor.java`

**核心方法**：

```java
public NodeSearchAction[] willTheSubTreeFurtherSearch(
        Node node, Metric metric, IndexObject query, double radius, 
        double[] queryToPivotDistance)
```

**剪枝逻辑**：

```java
NodeSearchAction[] actions = new NodeSearchAction[2];
Arrays.fill(actions, NodeSearchAction.RESULTUNKNOWN);

// 如果d(q, p1) - d(q, p2) > 2*radius，排除左子树
if(queryToPivotDistance[0] - queryToPivotDistance[1] > 2*radius)
    actions[0] = NodeSearchAction.RESULTNONE;

// 如果d(q, p2) - d(q, p1) > 2*radius，排除右子树
if(queryToPivotDistance[1] - queryToPivotDistance[0] > 2*radius)
    actions[1] = NodeSearchAction.RESULTNONE;

return actions;
```

#### 6.6.2 VPRangeCursor

**位置**：`index/search/VPRangeCursor.java`

**剪枝逻辑**：

```java
for (int i = 0; i < aNode.getNumChildren(); i++) {
    // 获取子树的距离范围
    double[][] range = aNode.getChildPredicate(i);
    
    for (int j = 0; j < aNode.getNumPivots(); j++) {
        // 如果d(q, pj) + range[1][j] <= radius，子树全部包含
        if (range[1][j] + queryToPivotDistance[j] <= radius) {
            nodeSearchActions[i] = NodeSearchAction.RESULTALL;
            break;
        }
        
        // 如果d(q, pj) + radius < range[0][j] 或 
        //    d(q, pj) - radius > range[1][j]，排除子树
        if (queryToPivotDistance[j] + radius < range[0][j] || 
            queryToPivotDistance[j] - radius > range[1][j]) {
            nodeSearchActions[i] = NodeSearchAction.RESULTNONE;
            break;
        }
    }
}
```

### 6.7 关键差异总结

| 特性 | 本项目实现 | UMAD-OriginalCode |
|-----|----------|-------------------|
| 接口设计 | 简化的Index接口 | 复杂的AbstractIndex |
| 节点结构 | TreeNode接口 | Node接口 |
| 配置管理 | TreeConfig类 | 构造函数参数 |
| 树高控制 | TreeHeightController | 内置在buildTree中 |
| pivot选择 | 统一的PivotSelector | 分散在各个类中 |
| 查询接口 | rangeQuery/knnQuery方法 | Cursor模式 |

**本项目的简化**：

1. 更清晰的接口设计
2. 统一的配置管理
3. 独立的树高控制器
4. 简化的查询接口
5. 更好的可测试性

---

## 七、参考文献

### 7.1 教材参考

1. 《大数据泛构》毛睿 著
   - 第3.4节：超平面树（General Hyper-plane Tree）
   - 第3.5节：优势点树（Vantage Point Tree）
   - 第5章：支撑点选择
   - 第6章：数据划分

### 7.2 经典论文

1. **GH树原始论文**：
   - Uhlmann, J. K. (1991). Satisfying general proximity/similarity queries with metric trees. Information processing letters, 40(4), 175-179.

2. **VP树原始论文**：
   - Yianilos, P. (1993). Data structures and algorithms for nearest neighbor search in general metric spaces.

3. **Pivot选择**：
   - Bustos, B., Navarro, G., & Chávez, E. (2003). Pivot selection techniques for proximity searching in metric spaces. Pattern Recognition Letters, 24(14), 2357-2366.

---

## 八、总结

本文档详细整理了GH树和VP树的理论基础，包括：

1. **树状索引的基本思想**：层次化剪枝、递归划分
2. **GH树**：超平面划分、双pivot、剪枝规则
3. **VP树**：球形划分、单pivot、距离范围剪枝
4. **理论对比**：划分方式、支撑点使用、性能特性
5. **Pivot选择**：随机、FFT、最大分散度
6. **代码实现**：UMAD-OriginalCode的实现分析

**关键要点**：

1. GHT和VPT各有优劣，没有绝对的优劣之分
2. 实际性能取决于数据分布、维度、查询模式等
3. Pivot选择对性能影响很大
4. 需要通过实验验证理论分析

**下一步**：

基于这些理论基础，我们将：

1. 实现GHTree和VPTree
2. 设计性能对比实验
3. 分析实验结果
4. 验证理论预测

---

**文档版本**：v1.0  
**最后更新**：2025-12-10  
**维护者**：Jixiang Ding
