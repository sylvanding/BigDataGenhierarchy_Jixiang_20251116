# 理论基础整理

本文档整理了项目所需的核心理论知识，来源于课程教材full.md和数据集介绍文档。

## 一、度量空间基本概念

### 1.1 度量空间定义

**来源**: full.md 第1章

**定义**: 度量空间是一个二元组 (S, d)，其中：

- S 是数据对象的集合
- d: S × S → ℝ 是距离函数（度量函数）

**度量函数的三大基本性质**:

1. **非负性 (Non-negativity)**:

   ```
   d(x, y) ≥ 0 对所有 x, y ∈ S
   d(x, y) = 0 当且仅当 x = y
   ```

2. **对称性 (Symmetry)**:

   ```
   d(x, y) = d(y, x) 对所有 x, y ∈ S
   ```

3. **三角不等性 (Triangle Inequality)**:

   ```
   d(x, z) ≤ d(x, y) + d(y, z) 对所有 x, y, z ∈ S
   ```

### 1.2 通用数据管理的意义

**核心思想**:

- 将不同类型的数据（向量、序列、图像等）抽象为度量空间中的对象
- 设计通用的算法处理多种数据类型
- 降低开发和维护成本，提高系统性价比

**优势**:

- 代码复用性高
- 易于扩展新数据类型
- 统一的查询接口

## 二、常见度量空间实例

### 2.1 向量空间

**来源**: full.md 第2.1节

#### 2.1.1 闵可夫斯基距离 (Minkowski Distance)

**定义**: 对于两个n维向量 x = (x₁, x₂, ..., xₙ) 和 y = (y₁, y₂, ..., yₙ)，它们之间的Lp距离定义为：

```
Lp(x, y) = (Σᵢ₌₁ⁿ |xᵢ - yᵢ|ᵖ)^(1/p),  p ≥ 1
```

**特殊情况**:

1. **L₁距离 (曼哈顿距离, Manhattan Distance)**:

   ```
   L₁(x, y) = Σᵢ₌₁ⁿ |xᵢ - yᵢ|
   ```

   **几何意义**: 在城市街区中，沿坐标轴方向行走的距离

   **示例计算**:

   ```
   x = (0, 0), y = (3, 4)
   L₁(x, y) = |3-0| + |4-0| = 3 + 4 = 7
   ```

2. **L₂距离 (欧几里得距离, Euclidean Distance)**:

   ```
   L₂(x, y) = sqrt(Σᵢ₌₁ⁿ (xᵢ - yᵢ)²)
   ```

   **几何意义**: 两点之间的直线距离

   **示例计算**:

   ```
   x = (0, 0), y = (3, 4)
   L₂(x, y) = sqrt(3² + 4²) = sqrt(9 + 16) = sqrt(25) = 5
   ```

3. **L∞距离 (切比雪夫距离, Chebyshev Distance)**:

   ```
   L∞(x, y) = maxᵢ₌₁ⁿ |xᵢ - yᵢ|
   ```

   **几何意义**: 各维度差值的最大值

   **示例计算**:

   ```
   x = (1, 2, 3), y = (4, 1, 6)
   L∞(x, y) = max(|4-1|, |1-2|, |6-3|) = max(3, 1, 3) = 3
   ```

**度量空间性质验证**:

以L₂距离为例，验证三大性质：

1. 非负性: ✓ (平方和的平方根总是非负的)
2. 对称性: ✓ ((xᵢ - yᵢ)² = (yᵢ - xᵢ)²)
3. 三角不等性: ✓ (Minkowski不等式)

### 2.2 序列空间

**来源**: full.md 第2.2节

#### 2.2.1 序列比对距离

对于蛋白质序列、DNA序列等，常用**编辑距离**或**比对距离**。

**编辑距离 (Edit Distance)**:

- 定义: 将一个序列转换为另一个序列所需的最少编辑操作次数
- 操作类型: 插入、删除、替换
- 算法: 动态规划

**比对距离 (Alignment Distance)**:

- 基于替代矩阵（如mPAM矩阵）
- 考虑生物学意义（某些氨基酸替换代价更小）

#### 2.2.2 mPAM替代矩阵

**来源**: 1.2 数据集介绍.md 第1.2.5节

**定义**: mPAM (modified Point Accepted Mutation) 矩阵是一个21×21的对称矩阵，表示20种氨基酸（加上一个未知类别）之间的替代代价。

**氨基酸编码**:

```
A=0,  R=1,  N=2,  D=3,  C=4,  Q=5,  E=6,  G=7,  H=8,  I=9,
L=10, K=11, M=12, F=13, P=14, S=15, T=16, W=17, Y=18, V=19, OTHER=20
```

**mPAM矩阵** (部分示例):

```
      A   R   N   D   C   Q   E   G   H   I   ...
A     0   2   2   2   3   2   2   2   2   2
R     2   0   2   2   4   2   2   2   2   3
N     2   2   0   2   4   2   2   2   2   3
...
```

**物理意义**:

- 对角线元素为0: 相同氨基酸替代代价为0
- 相似氨基酸（如L和I）替代代价较小
- 差异大的氨基酸替代代价较大

**全局序列比对算法**:

给定两个序列 seq1[1..m] 和 seq2[1..n]，使用动态规划计算比对距离：

```
初始化:
  dp[0][0] = 0
  dp[i][0] = i * gap_penalty (for i = 1..m)
  dp[0][j] = j * gap_penalty (for j = 1..n)

递推公式:
  for i = 1 to m:
    for j = 1 to n:
      match = dp[i-1][j-1] + mPAM[seq1[i]][seq2[j]]
      delete = dp[i-1][j] + gap_penalty
      insert = dp[i][j-1] + gap_penalty
      dp[i][j] = min(match, delete, insert)

结果:
  distance = dp[m][n]
```

**示例计算**:

```
seq1 = "ARND"
seq2 = "ARHD"

比对过程:
  A - A: mPAM[0][0] = 0
  R - R: mPAM[1][1] = 0
  N - H: mPAM[2][8] = 2
  D - D: mPAM[3][3] = 0

总距离 = 0 + 0 + 2 + 0 = 2
```

## 三、相似性查询

**来源**: full.md 第3.1节

### 3.1 范围查询 (Range Query)

**定义**: 给定查询对象 q 和查询半径 r，找出所有与 q 距离不超过 r 的数据对象。

```
RQ(q, r) = {s ∈ S | d(q, s) ≤ r}
```

**算法 - 线性扫描**:

```
Algorithm: LinearScanRangeQuery
Input: dataset S, query object q, radius r, metric d
Output: result set R

1: R = ∅
2: for each s ∈ S:
3:   if d(q, s) ≤ r:
4:     R = R ∪ {s}
5: return R
```

**复杂度**:

- 时间复杂度: O(n)，其中n是数据集大小
- 距离计算次数: n

**示例**:

```
数据集: {(0,0), (1,0), (0,1), (3,4), (5,5)}
查询对象: q = (0,0)
查询半径: r = 1.5
距离函数: L₂

计算过程:
  d(q, (0,0)) = 0 ≤ 1.5 ✓
  d(q, (1,0)) = 1 ≤ 1.5 ✓
  d(q, (0,1)) = 1 ≤ 1.5 ✓
  d(q, (3,4)) = 5 > 1.5 ✗
  d(q, (5,5)) = 7.07 > 1.5 ✗

结果: {(0,0), (1,0), (0,1)}
```

### 3.2 k近邻查询 (k-Nearest Neighbor Query, kNN)

**定义**: 给定查询对象 q 和整数 k，找出距离 q 最近的 k 个数据对象。

```
kNN(q, k) = {k个最近邻 | ∀s ∈ kNN(q,k), ∀s' ∉ kNN(q,k): d(q,s) ≤ d(q,s')}
```

**算法 - 基于优先队列的线性扫描**:

```
Algorithm: LinearScanKNNQuery
Input: dataset S, query object q, k, metric d
Output: k nearest neighbors

1: maxHeap = ∅  // 最大堆，存储当前k个最近邻
2: for each s ∈ S:
3:   dist = d(q, s)
4:   if |maxHeap| < k:
5:     maxHeap.insert(s, dist)
6:   else if dist < maxHeap.top().distance:
7:     maxHeap.removeTop()
8:     maxHeap.insert(s, dist)
9: return maxHeap 转为按距离升序的列表
```

**复杂度**:

- 时间复杂度: O(n log k)
- 空间复杂度: O(k)
- 距离计算次数: n

**示例**:

```
数据集: {(0,0), (1,0), (0,1), (3,4), (5,5)}
查询对象: q = (0,0)
k = 3
距离函数: L₂

所有距离:
  d(q, (0,0)) = 0
  d(q, (1,0)) = 1
  d(q, (0,1)) = 1
  d(q, (3,4)) = 5
  d(q, (5,5)) = 7.07

按距离排序:
  1. (0,0), distance = 0
  2. (1,0), distance = 1
  3. (0,1), distance = 1
  4. (3,4), distance = 5
  5. (5,5), distance = 7.07

结果: 前3个，即 {(0,0), (1,0), (0,1)}
```

### 3.3 多样化k近邻查询 (Diversified kNN, dkNN)

**动机**: 传统kNN可能返回非常相似的对象（如都来自同一聚类），缺乏多样性。

**定义**: 在保证相关性的同时，最大化结果集的多样性。

**多样性度量**:

```
diversity(R) = minᵢ,ⱼ∈R,i≠j d(i, j)
```

即结果集中任意两个对象的最小距离。

**算法 - 贪心多样化选择**:

```
Algorithm: GreedyDiversifiedKNN
Input: dataset S, query q, k, diversity weight λ, metric d
Output: diversified k nearest neighbors

1: candidates = kNN(q, k*α)  // α > 1, 获取更多候选
2: R = ∅
3: R = R ∪ {距离q最近的候选}
4: while |R| < k and 还有候选:
5:   for each c in candidates \ R:
6:     score(c) = (1-λ) * (-d(q,c)) + λ * min_{r∈R} d(c,r)
7:   R = R ∪ {得分最高的候选}
8: return R
```

**参数说明**:

- λ ∈ [0, 1]: 多样性权重
  - λ = 0: 退化为传统kNN
  - λ = 1: 完全追求多样性
  - λ = 0.5: 平衡相关性和多样性

**示例**:

```
数据集（两个聚类）:
  聚类1 (围绕原点): (0,0), (0.1,0), (0,0.1)
  聚类2 (远离原点): (10,10), (10.1,10), (10,10.1)

查询对象: q = (0,0)
k = 3
λ = 0.8 (高多样性权重)

传统kNN结果（都来自聚类1）:
  1. (0,0), distance = 0
  2. (0.1,0), distance = 0.1
  3. (0,0.1), distance = 0.1

dkNN结果（包含两个聚类）:
  1. (0,0), distance = 0        [选择最近的]
  2. (10,10), distance = 14.14  [多样性得分高]
  3. (0.1,0), distance = 0.1    [平衡]

分析:
  - 传统kNN的平均成对距离: 约0.1
  - dkNN的平均成对距离: 约4.7
  - dkNN结果更多样化
```

## 四、Pivot Table索引

**来源**: full.md 第3.3节

### 4.1 基本概念

**核心思想**:

- 选择k个支撑点(pivots)
- 预计算并存储每个数据对象到每个支撑点的距离
- 利用三角不等式在查询时剪枝，减少距离计算

**数据结构**:

```
PivotTable {
  pivots: [p₁, p₂, ..., pₖ]          // k个支撑点
  data: [s₁, s₂, ..., sₙ]             // n个数据对象
  distanceTable: double[n][k]         // 距离表
  // distanceTable[i][j] = d(sᵢ, pⱼ)
}
```

**构建过程**:

```
Algorithm: BuildPivotTable
Input: dataset S, number of pivots k, metric d
Output: Pivot Table

1: 选择k个支撑点: pivots = selectPivots(S, k)
2: 初始化距离表: distanceTable[n][k]
3: for i = 1 to n:
4:   for j = 1 to k:
5:     distanceTable[i][j] = d(S[i], pivots[j])
6: return PivotTable(pivots, S, distanceTable)

构建复杂度: O(n * k) 次距离计算
```

### 4.2 基于三角不等式的剪枝

**三角不等式回顾**:

```
对于任意三点 x, y, z:
  d(x, z) ≤ d(x, y) + d(y, z)

等价形式:
  d(x, z) ≥ |d(x, y) - d(y, z)|
```

#### 4.2.1 排除规则 (Exclusion Rule)

**定理 3-1**: 对于度量空间中任意数据点 p, q, s，如果 |d(p,q) - d(p,s)| > r，那么 d(q,s) > r。

**证明**:

```
由三角不等式:
  d(q, s) ≥ |d(p, q) - d(p, s)|

如果 |d(p, q) - d(p, s)| > r，则:
  d(q, s) ≥ |d(p, q) - d(p, s)| > r

因此 d(q, s) > r，s不是查询结果
```

**几何意义** (来自full.md 图22):

以支撑点p为中心，数据点s位于距离p为d(p,s)的圆弧上。查询对象q距离p为d(p,q)。

- 如果 d(p,q) - d(p,s) > r: s的圆弧在q的查询圆之外
- 如果 d(p,s) - d(p,q) > r: s的圆弧在q的查询圆之外
- 两种情况合并: |d(p,q) - d(p,s)| > r => 可以排除s

**示例**:

```
支撑点: p
数据点: s, d(p, s) = 5
查询对象: q, d(p, q) = 10
查询半径: r = 3

判断:
  |d(p,q) - d(p,s)| = |10 - 5| = 5 > 3 ✓

结论: 可以排除s，无需计算d(q,s)
```

#### 4.2.2 包含规则 (Inclusion Rule)

**定理 3-2**: 对于度量空间中任意数据点 p, q, s，如果 d(p,s) + d(p,q) ≤ r，那么 d(q,s) ≤ r。

**证明**:

```
由三角不等式:
  d(q, s) ≤ d(q, p) + d(p, s) = d(p, q) + d(p, s)

如果 d(p, q) + d(p, s) ≤ r，则:
  d(q, s) ≤ d(p, q) + d(p, s) ≤ r

因此 d(q, s) ≤ r，s一定是查询结果
```

**几何意义** (来自full.md 图24):

如果支撑点p本身距离q很近（d(p,q)小），且数据点s也距离p很近（d(p,s)小），那么s必然在q的查询圆内。

**示例**:

```
支撑点: p
数据点: s, d(p, s) = 2
查询对象: q, d(p, q) = 1
查询半径: r = 4

判断:
  d(p,q) + d(p,s) = 1 + 2 = 3 ≤ 4 ✓

结论: 可以直接包含s，无需计算d(q,s)
```

### 4.3 基于Pivot Table的范围查询

**算法** (来自full.md 表3):

```
Algorithm: PivotTableRangeSearch
Input: PivotTable PT, query object q, radius r, metric d
Output: result set R

1: R = ∅
2: // 预计算q到所有支撑点的距离
3: dpq[] = new double[k]
4: for j = 1 to k:
5:   dpq[j] = d(q, PT.pivots[j])
6:   if dpq[j] ≤ r:
7:     R = R ∪ {PT.pivots[j]}  // 支撑点也可能是结果
8:
9: // 对每个数据对象，尝试剪枝
10: for i = 1 to n:
11:   pruned = false
12:   included = false
13:   for j = 1 to k:
14:     dps = PT.distanceTable[i][j]  // d(pivot[j], data[i])
15:     
16:     // 包含规则
17:     if dpq[j] + dps ≤ r:
18:       R = R ∪ {PT.data[i]}
19:       included = true
20:       break
21:     
22:     // 排除规则
23:     if |dpq[j] - dps| > r:
24:       pruned = true
25:       break
26:   
27:   // 无法剪枝，计算实际距离
28:   if not pruned and not included:
29:     if d(q, PT.data[i]) ≤ r:
30:       R = R ∪ {PT.data[i]}
31:
32: return R
```

**复杂度分析**:

- 预处理: k次距离计算（q到pivots）
- 对每个数据:
  - 最好情况: O(1) 剪枝判断（使用第一个pivot就能剪枝）
  - 最坏情况: 1次距离计算（无法剪枝）
- 总距离计算: k + α*n，其中α∈[0,1]是未能剪枝的比例

**性能指标**:

```
剪枝率 = (被剪枝的数据数量 / 总数据数量) * 100%

距离计算节省率 = 1 - (实际距离计算次数 / n)
```

**示例**:

```
数据集大小: n = 1000
支撑点数量: k = 10
查询结果数量: 50

线性扫描:
  距离计算次数: 1000

Pivot Table:
  预处理: 10次（q到pivots）
  剪枝: 700个数据
  包含: 30个数据
  需要计算: 270个数据
  总距离计算: 10 + 270 = 280
  剪枝率: 73%
  性能提升: 1000/280 = 3.57倍
```

### 4.4 基于Pivot Table的kNN查询

**关键改进**: 使用动态查询半径

```
Algorithm: PivotTableKNNSearch
Input: PivotTable PT, query object q, k, metric d
Output: k nearest neighbors

1: maxHeap = ∅  // 存储当前k个最近邻
2: currentRadius = +∞  // 当前查询半径（动态更新）
3:
4: // 预计算q到所有支撑点的距离
5: dpq[] = new double[k]
6: for j = 1 to k:
7:   dpq[j] = d(q, PT.pivots[j])
8:
9: // 对每个数据对象
10: for i = 1 to n:
11:   pruned = false
12:   for j = 1 to k:
13:     dps = PT.distanceTable[i][j]
14:     
15:     // 使用当前半径进行排除
16:     if |dpq[j] - dps| > currentRadius:
17:       pruned = true
18:       break
19:   
20:   // 无法剪枝，计算实际距离
21:   if not pruned:
22:     dist = d(q, PT.data[i])
23:     if |maxHeap| < k:
24:       maxHeap.insert(PT.data[i], dist)
25:       if |maxHeap| == k:
26:         currentRadius = maxHeap.top().distance
27:     else if dist < currentRadius:
28:       maxHeap.removeTop()
29:       maxHeap.insert(PT.data[i], dist)
30:       currentRadius = maxHeap.top().distance
31:
32: return maxHeap 转为按距离升序的列表
```

**关键点**:

1. 查询半径动态缩小: 随着找到更近的邻居，currentRadius不断缩小
2. 剪枝效果增强: 查询半径越小，剪枝效果越好
3. 与范围查询的区别:
   - 范围查询的半径是固定的
   - kNN查询的半径是动态的

## 五、支撑点选择

**来源**: full.md 第5章

### 5.1 支撑点选择的重要性

**问题**: 如何选择k个支撑点以最大化Pivot Table的查询性能？

**影响因素**:

1. 支撑点的分布: 应该覆盖整个数据空间
2. 支撑点之间的距离: 应该尽可能远离
3. 支撑点的代表性: 应该能代表数据的分布特征

### 5.2 常见支撑点选择算法

#### 5.2.1 随机选择 (RANDOM)

**算法**:

```
Algorithm: RandomPivotSelection
Input: dataset S, number of pivots k
Output: k pivots

1: pivots = ∅
2: candidates = S的副本
3: for i = 1 to k:
4:   随机选择一个candidate
5:   pivots = pivots ∪ {candidate}
6:   从candidates中移除该candidate
7: return pivots
```

**优点**:

- 简单快速
- 无额外距离计算

**缺点**:

- 可能选到聚集的点
- 性能不稳定

#### 5.2.2 最远优先遍历 (FFT, Farthest-First Traversal)

**算法**:

```
Algorithm: FFTPivotSelection
Input: dataset S, number of pivots k, metric d
Output: k pivots

1: pivots = ∅
2: 随机选择第一个pivot: p₁
3: pivots = pivots ∪ {p₁}
4:
5: for i = 2 to k:
6:   maxMinDist = -1
7:   farthest = null
8:   
9:   for each s ∈ S \ pivots:
10:     // 计算s到已选pivots的最小距离
11:     minDist = min_{p ∈ pivots} d(s, p)
12:     
13:     // 选择最小距离最大的点
14:     if minDist > maxMinDist:
15:       maxMinDist = minDist
16:       farthest = s
17:   
18:   pivots = pivots ∪ {farthest}
19:
20: return pivots
```

**思想**: 每次选择距离已选支撑点最远的点

**优点**:

- 支撑点分布均匀
- 覆盖整个数据空间
- 查询性能通常较好

**缺点**:

- 构建开销较大: O(n * k²) 次距离计算

**示例**:

```
数据集: 2维向量
  A = (0, 0)
  B = (1, 0)
  C = (0, 1)
  D = (10, 10)
  E = (11, 10)
  F = (10, 11)

选择3个支撑点:

第1个: 随机选择A = (0, 0)
pivots = {A}

第2个: 找距离A最远的
  d(A, B) = 1
  d(A, C) = 1
  d(A, D) = 14.14 ← 最远
  d(A, E) = 15.03
  d(A, F) = 15.03
选择E = (11, 10)
pivots = {A, E}

第3个: 找距离{A, E}最远的
  对于B: min(d(A,B), d(E,B)) = min(1, 14.14) = 1
  对于C: min(d(A,C), d(E,C)) = min(1, 15.03) = 1
  对于D: min(d(A,D), d(E,D)) = min(14.14, 1) = 1
  对于F: min(d(A,F), d(E,F)) = min(15.03, 1.41) = 1.41 ← 最大
选择F = (10, 11)
pivots = {A, E, F}

结果: 三个支撑点分别位于数据空间的不同区域
```

#### 5.2.3 中心选择 (CENTER)

**思想**: 选择靠近数据集"中心"的点作为支撑点

**算法**:

```
1. 计算数据集的质心（centroid）
2. 选择距离质心最近的k个点作为支撑点
```

#### 5.2.4 边界选择 (BORDER)

**思想**: 选择位于数据空间边界的点作为支撑点

### 5.3 支撑点数量的选择

**原则**:

1. 支撑点过少: 剪枝效果不足
2. 支撑点过多:
   - 剪枝判断开销增大
   - 存储开销增大
   - 构建时间增长

**经验值**: k = sqrt(n) 或 k = log(n)

**实验确定**:

- 在特定数据集和查询负载下
- 测试不同k值的性能
- 选择性能最优的k

## 六、数据集格式

### 6.1 Vector数据集格式

**来源**: 1.2 数据集介绍.md 第1.2.1节

**文件格式**:

```
第一行: 维度 数据数量
后续每行: 坐标1 坐标2 ... 坐标n (空白分隔)
```

**示例**:

```
2 1000000
0.875313982141      0.649721892451
0.765359526559      0.718301991477
0.99627786558       0.0594818895136
...
```

**解释**:

- 第一行 "2 1000000": 2维向量，共100万个
- 后续每行是一个向量的坐标

**可用数据集**:

1. Uniform 20-d vector: 20维均匀分布，100万个
2. Uniform 5-d vector: 5维均匀分布，100万个
3. Clustered vector: 2维聚类数据，10万个（100个聚类，每个1000个点）
4. Texas: 德克萨斯州边界数据，194,724个点
5. Hawaii: 夏威夷边界数据，9,290个点

**距离函数**: L-metric (L₁, L₂, L∞等)

**建议查询半径**:

- 合成数据集: 0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3
- 边界数据集: 0, 0.02, 0.04, 0.06, 0.08, 0.1

### 6.2 Protein数据集格式

**来源**: 1.2 数据集介绍.md 第1.2.5节

**文件格式** (FASTA格式):

```
>序列ID和描述信息
氨基酸序列（可以跨多行）
>下一个序列ID和描述信息
氨基酸序列
...
```

**示例**:

```
>gi|798902 (Z49209) Tpi1p [Saccharomyces cerevisiae]
MARTFFVGGNFKLNGSKQSIKEIVERLNTASIPENVEVVICPPATYLDYSVSLVKKPQVTVGAQNAYLKA
SGAFTGENSVDQIKDVGAKWVILGHSERRSYFHEDDKFIADKTKFALGQGVGVILCIGETLEEKKAGKTL
DVVERQLNAVLEEVKDWTNVVVAYEPVWAIGTGLAATPEDAQDIHASIRKFLASKLGDKAASELRILYGG
SANGSNAVTFKDKADVDGFLVGGASLKPEFVDIINSRN
>gi|1015707 (Z49548) ORF YJR048w [Saccharomyces cerevisiae]
MTEFKAGSAKKGATLFKTRCLQCHTVEKGGPHKVGPNLHGIFGRHSGQAEGYSYTDANIKKNVLWDENNM
SEYLTNPKKYIPGTKMAFGGLKKEKDRNDLITYLKKACE
```

**格式说明**:

- 以 ">" 开头的行是描述信息
- 其他行是序列数据
- 连续的序列行应该拼接在一起

**20种氨基酸**:

```
A - Alanine      (丙氨酸)
R - Arginine     (精氨酸)
N - Asparagine   (天冬酰胺)
D - Aspartic acid (天冬氨酸)
C - Cysteine     (半胱氨酸)
Q - Glutamine    (谷氨酰胺)
E - Glutamic acid (谷氨酸)
G - Glycine      (甘氨酸)
H - Histidine    (组氨酸)
I - Isoleucine   (异亮氨酸)
L - Leucine      (亮氨酸)
K - Lysine       (赖氨酸)
M - Methionine   (蛋氨酸)
F - Phenylalanine (苯丙氨酸)
P - Proline      (脯氨酸)
S - Serine       (丝氨酸)
T - Threonine    (苏氨酸)
W - Tryptophan   (色氨酸)
Y - Tyrosine     (酪氨酸)
V - Valine       (缬氨酸)

另外: B, Z, U, X 表示未知或特殊氨基酸
```

**数据处理**:

1. 读取FASTA文件，提取序列
2. 将长序列切分为6-mers（长度为6的片段）
3. 对6-mers计算比对距离

**距离函数**: 基于mPAM的全局比对距离

**建议查询半径**: 0, 1, 2, 3, 4, 5, 6

## 七、性能评估指标

### 7.1 效率指标

1. **查询时间 (Query Time)**:
   - 定义: 执行一次查询所需的时间
   - 单位: 毫秒 (ms)

2. **距离计算次数 (Distance Calculations)**:
   - 定义: 执行查询时调用距离函数的次数
   - 是主要的计算开销

3. **剪枝率 (Pruning Rate)**:

   ```
   剪枝率 = (被剪枝的数据数量 / 总数据数量) * 100%
   ```

   - 越高越好，表示索引效果越好

4. **索引构建时间 (Index Build Time)**:
   - 定义: 构建索引所需的时间
   - 单位: 毫秒 (ms)

5. **索引大小 (Index Size)**:

   ```
   Pivot Table大小 = n * k * sizeof(double)
   ```

   其中n是数据集大小，k是支撑点数量

### 7.2 效果指标

1. **查全率 (Recall)**:

   ```
   Recall = |返回的正确结果| / |所有正确结果|
   ```

   - 对于精确查询，Recall应该为100%

2. **准确率 (Precision)**:

   ```
   Precision = |返回的正确结果| / |返回的所有结果|
   ```

   - 对于精确查询，Precision应该为100%

3. **加速比 (Speedup)**:

   ```
   Speedup = 线性扫描时间 / 索引查询时间
   ```

   - 衡量索引的性能提升

## 八、实验设计要点

### 8.1 对照实验

**原则**: 控制变量法

**示例**: 研究支撑点数量的影响

- 固定变量: 数据集、查询集、选择策略
- 变化变量: 支撑点数量 k = 5, 10, 15, 20, 25, 30
- 观测指标: 查询时间、距离计算次数、剪枝率

### 8.2 重复实验

**原则**: 多次重复取平均，减少随机误差

**建议**:

- 每个配置执行10-20次查询
- 报告平均值和标准差
- 剔除异常值

### 8.3 可视化

**图表类型**:

1. 折线图: 展示随参数变化的趋势
2. 柱状图: 对比不同方法的性能
3. 散点图: 展示数据分布
4. 表格: 详细的数值结果

**示例标题**:

- "支撑点数量对查询时间的影响"
- "不同支撑点选择策略的性能对比"
- "线性扫描 vs Pivot Table性能对比"

## 九、常见问题

### Q1: 如何验证距离函数满足度量空间性质？

**答案**:

1. 非负性: 显然（距离总是>=0）
2. 对称性: 验证 d(x,y) = d(y,x)
3. 三角不等性: 验证 d(x,z) <= d(x,y) + d(y,z)

### Q2: Pivot Table一定能提升性能吗？

**答案**: 不一定

- 如果数据分布特殊（如所有点都很近），剪枝效果可能不明显
- 如果支撑点选择不当，性能可能不如线性扫描
- 需要根据实际数据集选择合适的参数

### Q3: 如何选择最优的支撑点数量？

**答案**:

1. 理论指导: k = sqrt(n) 或 k = log(n)
2. 实验确定: 在目标数据集上测试不同k值
3. 权衡: 构建开销 vs 查询性能

### Q4: 为什么使用6-mers而不是完整蛋白质序列？

**答案**:

1. 完整序列长度不一，难以直接比较
2. 6-mers长度固定，便于比对
3. 6个氨基酸已经包含足够的生物学信息
4. 计算效率高

## 十、参考文献

1. full.md - 大数据泛构：一度量空间数据管理分析初探
2. 1.2 数据集介绍.md - UMAD数据集格式说明
3. UMAD-OriginalCode - 参考实现代码
